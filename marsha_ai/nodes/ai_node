#!/usr/bin/env python3

import gym
from stable_baselines3 import PPO
from marsha_ai.gym_env import MarshaGym
from marsha_ai.catch_interface import CatchInterface
from marsha_ai.callbacks import TensorboardCallback
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.callbacks import EvalCallback

from stable_baselines3.common.utils import get_device

from marsha_ai.util import func_timer

import sys
import rospy

import os
import time


def train(cont):
    MODEL_DIR = rospy.get_param("/training_dirs/model_dir")
    LOG_DIR = rospy.get_param("/training_dirs/log_dir")

    interface = CatchInterface()
    env = MarshaGym(interface)

    rospy.loginfo("Checking env...")
    check_env(env)
    rospy.loginfo("Env check complete!")

    callback_list = []

    # Save 10 models evenly over the course of training session
    if rospy.get_param("/hyperparameters/checkpoint_frequency") > 0: # Else not using checkpoints
        checkpoint_callback = CheckpointCallback(save_freq=rospy.get_param("/hyperparameters/checkpoint_frequency"), save_path=MODEL_DIR)
        callback_list.append(checkpoint_callback)
        
    # Evaluate the model 5 times during the course of training session and save the best model
    if rospy.get_param("/hyperparameters/eval_frequency") > 0: # Else not evaluating
        eval_callback = EvalCallback(env, best_model_save_path=MODEL_DIR, log_path=LOG_DIR, eval_freq=rospy.get_param("/hyperparameters/eval_frequency"))
        callback_list.append(eval_callback)

    callback_list.append(TensorboardCallback())

    device = get_device(device='cuda') # Only uses CPU currently, needs to be fixed
    print('Training device: ', device)

    # Load or create model
    if cont:
        model = PPO.load(MODEL_DIR + 'best_model', env=env, tensorboard_log=LOG_DIR)
    else:
        model = PPO("MlpPolicy", env, verbose=1, tensorboard_log=LOG_DIR)

    
    rospy.loginfo("Learning..........")

    model.learn(total_timesteps=rospy.get_param("/hyperparameters/total_timesteps"), 
                                log_interval=rospy.get_param("/hyperparameters/log_freq"), callback=callback_list)
    rospy.loginfo("Done Learning!")

    model.save(MODEL_DIR + "PPO_Pickup")

    env.close()


def eval():
    MODEL_DIR = rospy.get_param("/training_dirs/model_dir")
    LOG_DIR = rospy.get_param("/training_dirs/log_dir")

    interface = CatchInterface()
    env = MarshaGym(interface)
    obs = env.reset()

    model = PPO.load(MODEL_DIR + "best_model", env=env)

    rospy.loginfo("Evaluating...")

    for i in range(100):
        # Note: Random reachable pose: moveGroupInterface::getRandomPose()
        action, _states = model.predict(obs, deterministic=True) # takes ~ 5ms
        obs, reward, done, info = env.step(action) # takes ~ 200ms

        if done:
            obs = env.reset()

    rospy.loginfo("Done Evaluating!")

    env.close()


if __name__ == '__main__':
    # Arguments in form of bool(train) bool(continue) bool(eval)
    if len(sys.argv) == 6 or len(sys.argv) == 4:
        if sys.argv[1] == "true": # I only had to do this because sys.argv is a string with "true" which cant be easily converted to bool
            train(sys.argv[2] == "true")
        if sys.argv[3] == "true":
            rospy.loginfo("Beginning evaluation...")
            eval()
    else:
        raise ValueError("Arguments must be in form of three bools representing 'train', 'continue', and 'eval' options")


