#!/usr/bin/env python3

import gym
import rospy

from stable_baselines3 import TD3
from stable_baselines3.common.callbacks import EvalCallback

from marsha_ai.catch_bandit.gym_env import MarshaGym
from marsha_ai.catch_bandit.catch_interface import CatchInterface
from marsha_ai.callbacks import TensorboardCallback


def train():
    MODEL_DIR = rospy.get_param("/training_dirs/jet2/model_dir")
    LOG_DIR = rospy.get_param("/training_dirs/jet2/log_dir")

    interface = CatchInterface()
    env = MarshaGym(interface)

    callback_list = []
    callback_list.append(TensorboardCallback())

    if rospy.get_param("/hyperparameters/eval_frequency") > 0: # Else not evaluating
        eval_callback = EvalCallback(env, best_model_save_path=MODEL_DIR, log_path=LOG_DIR,
                                    deterministic=True,
                                    eval_freq=rospy.get_param("/hyperparameters/eval_frequency"),
                                    n_eval_episodes=rospy.get_param('/hyperparameters/n_eval_ep'))
    callback_list.append(eval_callback)

    #model = TD3("MlpPolicy", env, verbose=1, tensorboard_log=LOG_DIR)
    model = TD3.load(MODEL_DIR + 'best_model', env=env, tensorboard_log=LOG_DIR)
    model.learn(total_timesteps=rospy.get_param("/hyperparameters/total_timesteps"), callback=callback_list)

    model.save(MODEL_DIR + "TD3_Catch")


if __name__ == "__main__":
    train()
    print("Done.")